{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLucX5x/GnkBCbO79efMZg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import relevant packages"
      ],
      "metadata": {
        "id": "ascYiYqQc7Tk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VhpghOMcc10K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the data"
      ],
      "metadata": {
        "id": "P7u0HQ-ldMva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_dataset , mnist_info = tfds.load(name = 'mnist' , with_info = True , as_supervised = True , try_gcs=True)"
      ],
      "metadata": {
        "id": "3fiXwsCsdPNM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract train , validation and test dataset"
      ],
      "metadata": {
        "id": "ASy7yGvMedAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train , mnist_test = mnist_dataset['train'] , mnist_dataset['test']\n",
        "\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples #gives num of training set samples\n",
        "num_validation_samples = tf.cast(num_validation_samples , tf.int64) #makes sure its integar and not float\n",
        "\n",
        "num_test_samples = mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast( num_test_samples, tf.int64)\n",
        "\n"
      ],
      "metadata": {
        "id": "NgAXyY0beftg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling the data"
      ],
      "metadata": {
        "id": "c_Sl3UC3vBsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale(image , label) :\n",
        "  image = tf.cast(image , tf.float32)\n",
        "  image/= 255. #making the values between 0-255 into 0's and 1's\n",
        "  return image , label\n",
        "\n",
        "scaled_train_and_validation_dataset = mnist_train.map(scale)\n",
        "test_data = mnist_test.map(scale)\n"
      ],
      "metadata": {
        "id": "HKW3ZQfQvD4_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffling the data\n",
        "to make batches"
      ],
      "metadata": {
        "id": "cu_-Iol-wIv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size = 10000\n",
        "\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_dataset.shuffle(buffer_size)"
      ],
      "metadata": {
        "id": "mnJILCn9wilG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract train and validation dataset"
      ],
      "metadata": {
        "id": "tan45KNQx0lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n"
      ],
      "metadata": {
        "id": "UkJtN8_3x2hF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making batches"
      ],
      "metadata": {
        "id": "UV2tBHNWzQDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "train_data = train_data.batch(batch_size)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs , validation_targets = next(iter(validation_data))"
      ],
      "metadata": {
        "id": "E_-qf0F_zRrA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MODEL"
      ],
      "metadata": {
        "id": "RzIEg92m2RW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outline the model"
      ],
      "metadata": {
        "id": "ZBXUrMfF2WR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 100 #arbitrary\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([                                                                           #stacks layers\n",
        "\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)) ,\n",
        "                            tf.keras.layers.Dense(hidden_layer_size , activation='relu') , \n",
        "                            tf.keras.layers.Dense(hidden_layer_size , activation ='relu') ,\n",
        "                            tf.keras.layers.Dense(output_size , activation = 'softmax')\n",
        "\n",
        "])                 "
      ],
      "metadata": {
        "id": "t9m1s59v2Vrv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Choose the optimizer and the loss function"
      ],
      "metadata": {
        "id": "m1j-BqUH5_Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics =['accuracy'])   #sparse_categorical_crossentropy applies one hot encoding on the data\n"
      ],
      "metadata": {
        "id": "-qCBLTUo6DWI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "XEMiyJTL6tj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "model.fit(train_data , epochs = num_epochs , validation_data = (validation_inputs , validation_targets) , verbose =2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_U0Fq0w6vEZ",
        "outputId": "ebe84136-8777-4184-c2a6-9c4fdd76a96f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 6s - loss: 0.3332 - accuracy: 0.9040 - val_loss: 0.1665 - val_accuracy: 0.9507 - 6s/epoch - 11ms/step\n",
            "Epoch 2/5\n",
            "540/540 - 5s - loss: 0.1365 - accuracy: 0.9603 - val_loss: 0.1083 - val_accuracy: 0.9692 - 5s/epoch - 9ms/step\n",
            "Epoch 3/5\n",
            "540/540 - 5s - loss: 0.0949 - accuracy: 0.9717 - val_loss: 0.0834 - val_accuracy: 0.9755 - 5s/epoch - 9ms/step\n",
            "Epoch 4/5\n",
            "540/540 - 5s - loss: 0.0734 - accuracy: 0.9779 - val_loss: 0.0780 - val_accuracy: 0.9750 - 5s/epoch - 10ms/step\n",
            "Epoch 5/5\n",
            "540/540 - 5s - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.0652 - val_accuracy: 0.9780 - 5s/epoch - 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabf23cee50>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}